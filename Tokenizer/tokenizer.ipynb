{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "# list a byte object results in an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    \"\"\"\n",
    "    vocab and merges are obtained in training phase\n",
    "    encode and decodes works with the trained merges and vocab\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)} # int : bytes \n",
    "        self.merges = {} # pair : symbol\n",
    "        \n",
    "\n",
    "    # get the freq count of each pair in the string\n",
    "    def get_pair_freq(self, ids):\n",
    "        counts = {}\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "    # merge pair in the string with new idx\n",
    "    def merge(self, ids, pair, idx):\n",
    "        new_ids = [] # list of ints after merge\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "                new_ids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i += 1\n",
    "        return new_ids\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        # generate vocab and merges\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        num_merges = vocab_size - 256\n",
    "        for i in range(num_merges):\n",
    "            pair_freq = self.get_pair_freq(ids)\n",
    "            # get the pair with the maximum freq to merge first\n",
    "            pair = max(pair_freq, key=pair_freq.get)\n",
    "            idx = 256 + i\n",
    "            # print(f\"merging {pair} into a new token {idx}\")\n",
    "            ids = self.merge(ids, pair, idx)\n",
    "            self.merges[pair] = idx\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "        \n",
    "    def encode(self, text):\n",
    "        # a list of bytes\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        while (len(ids) >= 2):\n",
    "            freq_count = self.get_pair_freq(ids)\n",
    "            pair = min(freq_count, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            ids = self.merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # ids is a list of bytes \n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        # convert byte string back to human readable texts\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, my partner!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Mock Interview for Data Scientist Part 1: Problem Solving (45 minutes) Interviewer: Let’s start with a problem that we often encounter when working with clients. Imagine you’re working with a customer from the energy sector who wants to reduce their operational costs by optimizing their energy consumption. They have a large dataset of energy usage at different times of the day, spanning multiple years. They also have external data such as weather conditions, operational schedules, and energy prices. \t1.\tQuestion: How would you approach solving this problem? Follow-up: How would you engage with the customer to ensure\" \n",
    "tk = BasicTokenizer()\n",
    "tk.train(text, 500)\n",
    "# tk.decode(tk.encode(\"hello, my partner!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whatever', ' you', ' are', ' doing', ',', ' enjoy', ' it', '!']\n"
     ]
    }
   ],
   "source": [
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "gpt4pat = re.compile(GPT4_SPLIT_PATTERN)\n",
    "print(re.findall(gpt4pat, \"whatever you are doing, enjoy it!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new patterns as additional vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
